<!-- Generated by SpecStory -->

# Plan for extractors component using AI (2025-07-25 00:56Z)

_**User**_

lets plan the extractors domponent. I believe that this can be done using an ai agent with a search tool.
The agent can read the initial text and use the urls in that text to fint the locations that thuse urls are referring to. The search tool and the websited referred to can be used later for enrichment. 

---

_**Assistant**_

I'll help you plan the extractors component for the AI agent. Let me first examine the current structure and then design a comprehensive extraction system.


---

Read file: map_locations_ai/agent/extractors.py

---

Read file: map_locations/common/models.py

---

Now let me check the pipeline structure to understand how extractors will be integrated:


---

Read file: map_locations_ai/agent/pipeline.py

---

Now let me create a comprehensive plan for the extractors component. Based on your requirements, I'll design an AI agent-based extraction system that can:

1. Parse initial text to identify URLs
2. Use search tools to find location information
3. Extract location data from websites
4. Provide a foundation for enrichment


---

Cancelled

---

_**User**_

befor writing code. lets discuss the method.
here is an example starting data:
-------------------

Mon Aug 4
Landing 12pm
Stay at A32#Republique # Marais # 1BedRoom # Paris03 # AC
145 Rue du Temple, 3rd arr., 75003 Paris, France
(https://mail.google.com/mail/u/0/#search/booking+/FMfcgzQbdrRbHMDhSjXlfKZVCWFXzbKk )

17:00 – 19:00 Le Marais: Paris Hidden Gem, Jewish Quarter
Tour in English 🇬🇧 3 Individual travelers
Please meet us at: Saint-Paul - Rue Saint-Antoine Paris, Île-de-France 75004

Tue Aug 5

11am - Picasso Museum(order number 2312941 )

17:30-19:45 Montmartre Free Walking Tour
Tour in English 🇬🇧 3 Individual travelers
Please meet us at: Paris, Île-de-France In front of the Metro Blanche, the only exit, look for a yellow umbrella.


Wed Aug 6

11am - D’Orsey

17:30: Eiffel tower

Thu Aug 7
Disneyland Paris

Fri Aug 8




Sat Aug 9
10:30am Louis Vuitton David Hockney 25 (https://mail.google.com/mail/u/0/#inbox/FMfcgzQbfLVpMjtdgDsZrfRBvvrcFZTq)


Sun Aug 10




Mon Aug 11




Tue Aug 12
Eurostal leaves at 11:30am. Need to be there about an hour early (check)
Staying at Marriot Residence inn 181 - 183 Warwick Road, Kensington and Chelsea, London, W14 8PU, United Kingdom
Wed Aug 13




Thu Aug 14
Six, Start Time: 08:00 PM Booking ID: 24628197 SEAT INFO: Grand circle : B-8, B-9, B-10 Vaudeville Theatre, Strand, London, UK

Fri Aug 15
Warner Bros Harry potter studios (2 people) start at 14:30

Sat Aug 16




Sun Aug 17




Mon Aug 18




Tue Aug 19
15:15 Flights back


Places of interest:

D'Orsay Museum
Per laches cemetery
Jordan de tuilerie park
versailles palace
paris catacombs
jordan de luxembourg
batobus paris


Botanical gardens in Paris

https://www.atelier-lumieres.com/en
Parc des Buttes-Chaumont
pompidou centre 

Booking details Six:


Artist studios - 56 Rivoli, local 30 artists in their own space
flash invaders


https://www.musee-rodin.fr/en

https://www.centrepompidou.fr/en/

https://museepicassoparis.fr/

https://www.musee-orangerie.fr/en (Monet)

https://www.tourmontparnasse56.com/ Montparnasse Tower Obs Deck (Tour Montparnasse)

NYT article about the Impressionist: Art of Seeing and Being Seen
https://www.nytimes.com/interactive/2021/06/04/arts/design/berthe-morisot-in-england.html?auth=login-google1tap&login=google1tap


https://www.fondationlouisvuitton.fr/en/ticketing David Hockney 25

Place de la Concorde

Marais Tours (more options)

Covered passages

Musee Du Quai Branly Jacques Chirac

https://ticket.toureiffel.paris/en

https://paradoxmuseumparis.com/


מדריך לפאריז עם ילדים
https://www.metailimbaolam.com/%D7%A4%D7%A8%D7%99%D7%96-%D7%A2%D7%9D-%D7%99%D7%9C%D7%93%D7%99%D7%9D/


London:
East london spitalfield market
Borough market
Little venice - warwick avenue, camden lock, paddington st

Walking tour company - London Walks
Leadenhall market
Saachi gallery
Hampstead heath (flask walk) summit from parliament hill, bathing pond

Near london bridge - hays galleria
HMS Belfast (battleship - floating museum)

-------------
think about the agent and system prompt that will ensure a high degree of success - not lose and location that is mentioned or linked to and not add locations unless the text specifically asked fo additional locations (e.g. "- additionals best museums in London" should be interpreted as a specific invitation)

---

_**Assistant**_

<think><details><summary>Thought Process</summary>**Planning extraction method**

The user wants to focus on the design and method of the extractors component rather than the code, sharing sample itinerary data with relevant details. They’re looking to create an effective agent system prompt to ensure all locations are accurately extracted without adding unintended information.

To tackle this, I’ll plan the extraction method, draft prompts, and define pipeline steps and rules. First, I need to clarify input requirements, identify sources, and think through the extraction step without visiting external websites for enrichment.
**Refining extraction design**

I’m thinking about parsing the list of URLs as potential location candidates. For the system design, I’ll divide the text extraction process into several key steps. Pre-processing will segment lines, remove date or time lines, and identify bullet points. I’ll utilize Named Entity Recognition (NER) tools for location identification and regex for address patterns.

Next, URLs will be extracted, deduplicated, and normalized. My agent prompt must ensure that only explicitly mentioned locations are included, while additional suggestions are treated separately, and I’ll draft specific architecture for effective extraction.
**Designing extraction methods**

I’m considering how to structure the URL extractor. For each URL, I'll skip those from Gmail but might still look for booking details in mail threads. For other domains, I’ll fetch the title and possibly some metadata using a search approach. I’ll also use regex to derive location names from slugs and store results in a data class for future use.

To implement this, I’ll parse for URLs and location patterns, apply Named Entity Recognition, and summarize the plan. I'll draft a prompt to guide the agent on including explicit locations while parsing.</details></think>

---

Here is a concrete, step-by-step plan for the “extractors” component and the system-/assistant-prompts that will drive the LLM-powered extractor.

──────────────────────────────────
1.  OBJECTIVES & CONSTRAINTS
──────────────────────────────────
•  HIGH-RECALL, HIGH-PRECISION: Capture every location that is explicitly mentioned or linked, while avoiding hallucinated / “helpful extra” places.  
•  STRICT SOURCE TIE-BACK: Every location in the output must be traceable to (a) an exact text span or (b) a specific URL in the user’s input.  
•  NO PROACTIVE ADDITIONS: The only time we’d add unseen places is when the text explicitly invites it (e.g. “please add other museums…”).  
•  MINIMAL OUTPUT: For the extraction phase we only need the skeletal fields that will allow later enrichers/geocoders to work:
  – name (string, required – best guess / “unknown” if we’re not sure)  
  – address_or_hint (string, optional)  
  – source_type (“text” | “url”)  
  – source_snippet_or_url (string)  
  – confidence (float 0-1)  

──────────────────────────────────
2.  PIPELINE DESIGN
──────────────────────────────────
TextExtractor.extract(text)
  a.  Pre-processing
      • Split into lines, keep original ordering & indices.  
      • Regex-extract URLs early so we don’t feed them to the NER twice.  
      • Normalise whitespace, keep date/time lines for context (they’re sometimes addresses).  
  b.  Line-level heuristics
      • Regexes for full addresses: “\d+ .+?, \d{4,5} .+?”  
      • “meet us at:” → capture rest of line.  
      • “Staying at” / “Stay at” → capture remainder.  
  c.  Named-Entity Recognition
      • Run spaCy (en_core_web_sm is good enough) limited to entity labels {GPE, LOC, FAC}.  
      • For each entity keep (text, start_line, label, sentence) for later merging.  
  d.  Post-processing & merge
      • De-duplicate (case-insensitive, strip punctuation).  
      • Merge entities that appear in the same sentence (“Eiffel Tower” + “Paris” → keep Eiffel Tower, attach city=”Paris” as metadata if wanted).  
      • Assign confidence: heuristics > NER (0.9) / NER alone (0.7) / fuzzy (0.4).  

URLExtractor.extract(urls)
  a.  Classification
      • Gmail thread (mail.google.com) → keep but treat as low confidence until enrich stage.  
      • Other domains → candidate for scraping.  
  b.  Fast title fetch (requests + BeautifulSoup, 3-second timeout)
      • Parse <title> and common OG tags (og:title, twitter:title).  
      • Slug fallback: last path segment with “-” or “_” replaced by spaces.  
  c.  Produce same minimal record:  
      name           = cleaned title | slug | “unknown”  
      address_or_hint= “” (unknown until enrichment)  
      source_type    = “url”  
      source_snippet_or_url = full url  
      confidence     = 0.6 if title found else 0.3  

StructuredExtractor.extract(data)
  • Not needed yet – stub remains.

Combine & expose in pipeline:
  LocationPipeline.process_text() → TextExtractor → return list  
  LocationPipeline.process_urls() → URLExtractor → return list  
  LocationPipeline.process_mixed_input() just concatenates

──────────────────────────────────
3.  AGENT PROMPTS
──────────────────────────────────
System prompt for extraction-only agent
```
You are an extraction-only agent whose sole task is to pull out locations that are
explicitly named or linked in the user’s input.

Rules:
1.  Do NOT invent or add new venues unless the text *explicitly* asks for extra suggestions
    (e.g. lines starting with “additional”, “please suggest”, etc.).
2.  Every extracted item must include:
      • "name": best-guess text of the location (or "unknown")
      • "address_or_hint": address, meeting point, or empty string
      • "source_type": "text" or "url"
      • "source_snippet_or_url": the exact line or the URL itself
      • "confidence": 0-1 float
3.  Preserve ALL locations, even if they look repeated or have low confidence;
    de-duplication happens later downstream.
4.  Output JSON array ONLY. No extra commentary.
5.  Think step-by-step privately but show ONLY the final JSON.
```

Assistant (scratch-pad / internal) prompt template
```
### INTERNAL THOUGHTS (not visible to end user)
[Paste user input]
Step 1: list all URLs …
Step 2: line-by-line scan for "Stay at", "Meet us at" …
Step 3: apply NER …
Step 4: merge + build JSON …
ONLY AFTER processing -> return JSON.

### FINAL ANSWER
<json>
```
The *_internal_* part is not returned; we ensure the model confines its visible output to the JSON block.

──────────────────────────────────
4.  FLOW OF YOUR SAMPLE INPUT
──────────────────────────────────
Example extractions (abbreviated):
```
[
  { "name": "145 Rue du Temple", "address_or_hint": "145 Rue du Temple, 75003 Paris, France",
    "source_type": "text", "source_snippet_or_url": "145 Rue du Temple, 3rd arr., 75003 Paris, France",
    "confidence": 0.9 },

  { "name": "Saint-Paul - Rue Saint-Antoine Paris", "address_or_hint": "75004",
    "source_type": "text", "source_snippet_or_url": "Please meet us at: Saint-Paul - Rue Saint-Antoine Paris, Île-de-France 75004",
    "confidence": 0.85 },

  { "name": "Picasso Museum", "address_or_hint": "", "source_type": "text",
    "source_snippet_or_url": "11am - Picasso Museum(order number 2312941 )",
    "confidence": 0.8 },

  { "name": "https://www.fondationlouisvuitton.fr/en/ticketing", "address_or_hint": "",
    "source_type": "url", "source_snippet_or_url": "https://www.fondationlouisvuitton.fr/en/ticketing",
    "confidence": 0.6 },

  ...
]
```
Note that *“additional best museums in London”* would **not** trigger new items unless that exact phrase appears and we’ve built an “invitation detector”.

──────────────────────────────────
5.  NEXT IMPLEMENTATION TASKS
──────────────────────────────────
1.  Write the invitation-detector helper (simple keyword classifier).  
2.  Flesh out `TextExtractor.extract()` with regex + spaCy logic.  
3.  Implement fast-title fetch in `URLExtractor` using requests+bs4.  
4.  Unit tests: feed the sample text and assert that every obvious location appears.  
5.  Integrate into `LocationPipeline.process_text/process_urls`.

Once you’re happy with this plan, we can start coding the individual pieces.

---

_**User**_

write the plan for this step in a current wor md file for return referencing as we work through this. Usde this conversation to prepare that file. Note that most documents to be consumed would look like partial itineraries and a laundry list of places or something like a travel advice page.

---

_**Assistant**_

<think><details><summary>Thought Process</summary>**Creating a markdown file plan**

The user is asking me to write a plan in a markdown file for future reference as we progress. They want to focus on designing the extractors component, including system prompts and pipeline design. This markdown file should be stored in the repository, possibly named something like `docs/extractors_plan.md` or `project_plan_extractors.md`. It seems important to structure it clearly, considering that many documents will serve as partial itineraries or travel advice pages. I'll ensure it’s organized for easy referencing later!
**Planning file creation**

I need to create a markdown file with the plan details, like objectives, constraints, and pipeline design, and there isn't a file yet. I’ll follow the code style guidelines, ensuring lines are under 100 characters and using bullet lists. The file will be named `docs/extractors_plan.md`. I’ll include headings such as Title, Overview, Goals, and Implementation tasks, while ensuring to mention partial itineraries and examples. Let’s craft everything clearly before creating the file!</details></think>

---

<details><summary>Edit file: docs/extractors_plan.md</summary>

I will create a markdown document capturing the detailed plan for the Extractors component.

**Chunk 1**
Lines added: 139, lines removed: 1

```diff
@@ -1,1 +1,139 @@
- 
+ # Extractors Component – Detailed Plan
+ 
+ *Last updated: 2025-07-25*
+ 
+ ## 1. Context & Typical Inputs
+ 
+ Most source documents fall into one of two shapes:
+ 
+ 1. **Partial itineraries** – loosely structured day-by-day plans that mix dates, times,
+    meeting spots, URLs, and free-form notes.
+ 2. **Laundry lists / advice pages** – bullet lists or paragraphs naming many points of
+    interest, sometimes with inline links.
+ 
+ The extractor must succeed on both with **high recall** and **no hallucinated extra
+ locations**.
+ 
+ ---
+ 
+ ## 2. Objectives & Constraints
+ 
+ • **High-recall, high-precision**: capture every explicitly mentioned or linked location.
+ • **Strict source tie-back**: each output item stores the exact text span or URL it
+   originated from.
+ • **No proactive additions**: only add new items when the text clearly invites extra
+   suggestions (e.g. "additional best museums …").
+ • **Minimal schema for this phase**:
+ 
+ ```json
+ {
+   "name": string,               // best guess or "unknown"
+   "address_or_hint": string,    // may be empty
+   "source_type": "text" | "url",
+   "source_snippet_or_url": string,
+   "confidence": float           // 0-1
+ }
+ ```
+ 
+ ---
+ 
+ ## 3. Pipeline Design
+ 
+ ### 3.1 TextExtractor
+ 
+ 1. **Pre-processing**
+    • Split into lines, keep indices.
+    • Early regex grab of URLs to hand off to `URLExtractor`.
+ 2. **Line-level heuristics**
+    • Regex for addresses: `\d+ .+?, \d{4,5} .+?`.
+    • Key phrases: "Stay at", "Staying at", "meet us at", "Please meet us at".
+ 3. **Named-Entity Recognition**
+    • spaCy `en_core_web_sm`, entity labels {`GPE`, `LOC`, `FAC`}.
+ 4. **Post-processing & merge**
+    • Case-insensitive de-dupe.
+    • Merge nested entities ("Eiffel Tower" + "Paris").
+    • Confidence: heuristics 0.9, NER 0.7, fuzzy 0.4.
+ 
+ ### 3.2 URLExtractor
+ 
+ 1. **Classification**
+    • Gmail threads → low confidence until enrichment.
+ 2. **Fast title fetch** (≤3 s)
+    • `requests` + `BeautifulSoup` → `<title>`, `og:title`, `twitter:title`.
+    • Fallback: slug from path.
+ 3. **Record assembly**
+    • `confidence` 0.6 if title found, else 0.3.
+ 
+ ### 3.3 StructuredExtractor
+ 
+ *Stub for future CSV/JSON/XML inputs.*
+ 
+ ### 3.4 Pipeline Glue
+ 
+ ```text
+ LocationPipeline.process_text()  -> TextExtractor
+ LocationPipeline.process_urls()  -> URLExtractor
+ LocationPipeline.process_mixed() -> concat(text, urls)
+ ```
+ 
+ ---
+ 
+ ## 4. LLM-Agent Prompts
+ 
+ ### 4.1 System Prompt (extraction agent)
+ 
+ ```
+ You are an extraction-only agent. Your task is to list every location explicitly
+ named or linked in the user's input.
+ 
+ Rules:
+ 1. Do NOT invent locations unless the text explicitly asks for suggestions.
+ 2. For each item output:
+    • name, address_or_hint, source_type, source_snippet_or_url, confidence
+ 3. Preserve duplicates; de-duplication happens later.
+ 4. Output JSON array ONLY, no extra commentary.
+ ```
+ 
+ ### 4.2 Assistant Internal Scratch-Pad
+ 
+ ```
+ INTERNAL STEPS (not returned):
+ 1. List URLs
+ 2. Scan lines with heuristics
+ 3. Run spaCy NER
+ 4. Merge & build JSON
+ 
+ VISIBLE OUTPUT: <json>
+ ```
+ 
+ ---
+ 
+ ## 5. Sample Walk-Through
+ 
+ Given the itinerary snippet in the design discussion, expected extractions include
+ (but are not limited to):
+ 
+ - 145 Rue du Temple, 75003 Paris
+ - Saint-Paul – Rue Saint-Antoine, 75004 Paris
+ - Picasso Museum
+ - `https://www.fondationlouisvuitton.fr/en/ticketing`
+ - Eiffel Tower
+ 
+ Each record stores the exact line or URL and an appropriate confidence value.
+ 
+ ---
+ 
+ ## 6. Implementation Tasks
+ 
+ 1. Build **invitation detector** for phrases that request extra suggestions.
+ 2. Implement `TextExtractor.extract()` (regex + spaCy).
+ 3. Implement `URLExtractor.extract()` (title fetch & record build).
+ 4. Unit tests covering sample itinerary & list pages.
+ 5. Wire into `LocationPipeline` methods.
+ 
+ ---
+ 
+ ## 7. Performance Targets
+ 
+ • ≤30 s per 1 000-line document (incl. URL titles).  
+ • ≤500 MB peak memory during extraction. 
```

</details>

---

I’ve created `docs/extractors_plan.md` with the detailed design, prompts, and next-step checklist.  
We can now reference this file as we implement each task.

---

_**User**_

lets built this step as an agent that is given a system prompt and tools (searchn entity extraction, url extraction etc if they are expected tpo do better than the llm for a subtask). We can use openai agents. The Open AI API key woul;d be available in LAVI_OPENAI_KEY env var. update the plan accordingly

---

_**Assistant**_



---

<details><summary>Edit file: docs/extractors_plan.md</summary>

I will append a new section detailing the OpenAI agent architecture, tool definitions, and environment variable usage, and update the implementation task list.

**Chunk 1**
Lines added: 85, lines removed: undefined

```diff
@@ -138,2 +138,87 @@
  • ≤30 s per 1 000-line document (incl. URL titles).  
  • ≤500 MB peak memory during extraction. 
+ 
+ ---
+ 
+ ## 8. Agent Implementation (OpenAI Function-Calling)
+ 
+ ### 8.1 Overview
+ 
+ We will wrap the extractor logic inside an **OpenAI ChatCompletion agent** that can
+ call Python “functions” (as described in the OpenAI function-calling interface)
+ to delegate specific subtasks that classical code handles better than the LLM.
+ 
+ The agent runs in two tiers:
+ 
+ 1. **LLM Tier** – decides *which* tool to invoke and with *what* arguments, or
+    responds with the final JSON when all information is gathered.
+ 2. **Tool Tier** – deterministic Python functions that perform heavy-lifting
+    (regex, spaCy NER, HTTP fetches) and return structured data back to the LLM.
+ 
+ ### 8.2 Environment & Credentials
+ 
+ Set the OpenAI key via an env var (already configured in CI/dev machines):
+ 
+ ```bash
+ export LAVI_OPENAI_KEY="sk-..."  # done via direnv or .env
+ ```
+ 
+ The agent wrapper will read:
+ 
+ ```python
+ import os, openai
+ openai.api_key = os.environ["LAVI_OPENAI_KEY"]
+ ```
+ 
+ ### 8.3 Function Registry (Tools)
+ 
+ | Name                | Purpose                               | Python Impl | Exposed Args |
+ |---------------------|---------------------------------------|-------------|--------------|
+ | `extract_entities`  | spaCy-based NER on given text chunk   | `nlp_utils.py` | `text` |
+ | `regex_addresses`   | Heuristic address regexes             | `nlp_utils.py` | `text` |
+ | `invitation_check`  | Detect if text invites extra places   | `heuristics.py` | `text` |
+ | `url_title_fetch`   | Fast title/OG scrape (<3 s)           | `net_utils.py` | `url` |
+ | `slug_to_name`      | Derive name from URL slug             | `net_utils.py` | `url` |
+ 
+ These functions return JSON-serialisable dicts / lists so the LLM can reason
+ about them.
+ 
+ ### 8.4 Agent Flow (high-level)
+ 
+ ```mermaid
+ sequenceDiagram
+   participant U as User Input
+   participant A as Chat (LLM)
+   participant T as Tools
+   U->>A: raw itinerary text + urls
+   A->>T: regex_addresses(text)
+   T-->>A: address candidates
+   A->>T: extract_entities(text)
+   T-->>A: NER entities
+   alt URLs found
+     A->>T: url_title_fetch(url)
+     T-->>A: title & metadata
+   end
+   A-->>U: final JSON array of locations
+ ```
+ 
+ ### 8.5 System Prompt (updated)
+ 
+ Add a one-liner instructing the agent to use tools when helpful:
+ 
+ > "When a tool can accomplish the task more accurately or efficiently, return a
+ >  `function_call` with the proper name and arguments."
+ 
+ Everything else in §4.1 still applies.
+ 
+ ### 8.6 Implementation Notes
+ 
+ • Use `openai.ChatCompletion.create(model="gpt-4o-0613", …)` with
+   `functions=[…], function_call="auto"`.
+ • Stream responses disabled for simplicity; handle up to ~15 k tokens per call.
+ • Retry logic with exponential back-off (HTTP 429, 5xx).
+ 
+ ### 8.7 Updated Implementation Tasks (adds 6-7)
+ 
+ 6. Build tool wrappers (`nlp_utils.py`, `net_utils.py`, `heuristics.py`).
+ 7. Implement `agent/extraction_agent.py` that orchestrates ChatCompletion calls. 
```

</details>

---

The plan now includes Section 8, detailing how we will implement an OpenAI function-calling agent, the required tool wrappers, environment-variable usage ( `LAVI_OPENAI_KEY` ), and the updated task list.  
Let me know when you’d like to start coding the utilities or the agent wrapper.

---

_**User**_

if it helps, we can use the HF smolagents to make the tool using agent

---

_**Assistant**_

<think><details><summary>Thought Process</summary>**Considering tool integration options**

The user suggests that we might use Hugging Face SmolAgents for building the extraction agent wrapper. They haven't requested anything specific yet, but it seems like they're continuing the discussion. Since we have a plan in our documents, I could consider updating it to include this integration as an optional alternative. I wonder if I should append a note about SmolAgents after section 8.7 and possibly ask the user for their preference since they mentioned it might help.
**Updating implementation plan**

It seems the user is open to using HF SmolAgents! I think updating the plan to include the pros and cons of this approach is a great idea. I’ll create a new section, 8.8, titled "Alternate Implementation Using HF SmolAgents." Then, I need to generate a call to edit the document to make this addition. This way, we can clarify the benefits and drawbacks of this integration while keeping the user informed.</details></think>

---

<details><summary>Edit file: docs/extractors_plan.md</summary>

I will add an optional sub-section describing how HF SmolAgents could be used instead of the OpenAI function-calling wrapper.

**Chunk 1**
Lines added: 28, lines removed: undefined

```diff
@@ -223,2 +223,30 @@
  6. Build tool wrappers (`nlp_utils.py`, `net_utils.py`, `heuristics.py`).
  7. Implement `agent/extraction_agent.py` that orchestrates ChatCompletion calls. 
+ 
+ ### 8.8 Alternative: Hugging Face *smol-agents*
+ 
+ If we prefer to stay entirely in open-source-only tooling, we can swap the
+ OpenAI ChatCompletion layer for Hugging Face’s *smol-agents* framework
+ ( https://github.com/huggingface/smol-agents ).  The differences:
+ 
+ | Aspect                | OpenAI Function-Calling            | HF smol-agents                              |
+ |-----------------------|------------------------------------|---------------------------------------------|
+ | Model                 | `gpt-4o` via OpenAI API            | Any HF chat model (e.g. `mistralai/Mixtral`) |
+ | Tool Interface        | JSON `function_call` schema        | Python decorators (`@tool`)                 |
+ | Pricing / limits      | Pay-per-token, external service    | Local weights possible (GPU/CPU)            |
+ | Quality               | Typically higher (GPT-4)           | Depends on chosen model                     |
+ | Deployment simplicity | Minimal code, but needs API key    | Need to host/quantise model or use HF Inference API |
+ 
+ **Integration points:**
+ 
+ 1. Replace the `openai.ChatCompletion.create()` call with the smol-agents
+    `Agent` class, loading a chosen HF chat model (we can test with `HuggingFaceHub`).
+ 2. Register the same Python tool functions (`extract_entities`, `url_title_fetch`, …)
+    using the `@tool` decorator.
+ 3. Adjust the system prompt slightly (smol-agents uses `AGENT_INSTRUCTIONS`).
+ 
+ No further structural changes are needed—the `TextExtractor`, `URLExtractor`, and
+ pipeline remain identical.
+ 
+ *Decision pending:* we will prototype with OpenAI first for maximum quality, and
+ leave a feature flag `USE_SMOL_AGENTS=True` in `config.yaml` to switch later. 
```

</details>

---

Added subsection 8.8 explaining how we could swap the OpenAI layer for Hugging Face *smol-agents*, including a comparison table and integration notes.  
The plan now covers both options and leaves room for a future feature flag.

---

